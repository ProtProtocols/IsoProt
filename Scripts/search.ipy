"""
This module provides the complete functionality to run a MS/MS search
and convert the results into a TSV-formatted text file.

Input (search_in):
  :key workdir: Absolute path to the working directory to use
  :key fasta_db_path: Absolute path to the FASTA file to use
  :key generate_decoy: Boolean to indicate whether reversed decoy sequences should be generated.
  :key input_directory: Absolute path(s) to the directory(ies) containing the MS/MS spectra files (in MGF format)
  :key precursor_tolerance: The precursor tolerance in ppm
  :key fragment_tolerance: The fragment tolerance in Da
  :key labelling: The labelling PTM definition
  :key labelling_method: The labelling method's name
  :key missed_cleavages: Number of missed cleavages allowed (int)
  :key var_ptms: A list/duple of variable PTMs to include in the search
  :key fixed_ptms: A complete string definition of the fixed PTMs to use (not
                     including the labelling method).
  :key on_search_complete: If set, this function is called once the search is complete.
Output (search_out):
  :key result_files: Path to the (TSV) result files.
"""

import os
import time
import typing
import psutil
import subprocess

global search_in, search_out


def main():
    # make sure the input object exists
    if "search_in" not in globals():
        raise Exception("Missing required settings object 'search_in'")

    global search_in
    global search_out
    
    result_files = list()

    # run over spectra subfolders if necessary
    for dir in search_in["input_directory"]:
        
        curr_work_dir = search_in["workdir"]
        if len(search_in["input_directory"]) > 1:
            curr_work_dir = os.path.join(search_in["workdir"], os.path.basename(dir))
            # create the working directory if it doesn't yet exist
            if not os.path.exists(curr_work_dir):
                os.mkdir(curr_work_dir)

        # automatically launch the search
        result_file = run(
                      work_dir=curr_work_dir,
                      fasta_db_path=search_in["fasta_db"],
                      generate_decoy=search_in["generate_decoy"],
                      spectra_dir=dir,
                      precursor_tolerance=search_in["precursor_tolerance"],
                      fragment_tolerance=search_in["fragment_tolerance"],
                      labelling=search_in["labelling"],
                      labelling_method=search_in["labelling_method"],
                      missed_cleavages=search_in["missed_cleavages"],
                      var_ptms=search_in["var_ptms"],
                      fixed_ptms=search_in["fixed_ptms"])
                    
        result_files.append(result_file)

    # create the result object
    search_out = {"result_files": result_files}

    # call the user defined function if set
    if "on_search_complete" in search_in:
        search_in["on_search_complete"]()


def adapt_mgf_titles(filenames):
    """
    This function changes all MGF titles to [filename].[spec index]

    :param: filenames: Filenames of the MGF files to change
    """
    for filename in filenames:
        with open(filename, "r") as reader:
            clean_name = os.path.basename(filename).replace(" ", "_")
            # MGF index reference in PSI standard is 1-based
            cur_index = 1

            with open(filename + ".tmp", "w") as writer:
                for line in reader:
                    if line[0:6] == "TITLE=":
                        writer.write("TITLE=" + clean_name + "." + str(cur_index) + "\n")
                        cur_index += 1
                    else:
                        writer.write(line)

        # backup the original file
        os.rename(filename, filename + ".org")
        os.rename(filename + ".tmp", filename)


def filter_mgf_peaks(filenames, min_mz=100, max_mz=150):
    """
    Removes all peaks from the passed mgf files that are below min_mz or
    above max_mz. The results are written to files with the same name but
    ".filtered" appended to the name.

    :param: filenames: List of filenames to process.
    :param: min_mz: Minimum m/z a peak must have to be kept
    :param: max_mz: Maximum m/z a peak may have to be kept
    """
    for filename in filenames:
        with open(filename, "r") as reader:
            with open(filename + ".filtered", "w") as writer:
                for line in reader:
                    # check if it's a peak
                    if line[0].isdigit():
                        sep_index = line.find(" ")
                        if sep_index < 0:
                            sep_index = line.find("\t")
                        if sep_index < 0:
                            raise Exception("Invalid peak definition found: " + line +
                                            ". Failed to filter file " + filename)

                        mz = float(line[:sep_index])

                        # ignore any non-matching peaks
                        if mz < min_mz or mz > max_mz:
                            continue

                    # copy the line
                    writer.write(line)


class TaskRunner:
    def __init__(self, cwd):
        self.cwd = cwd

    def _write_log(self, entry):
        """
        Append to the default log file
        :param entry: The entry to add
        """
        if not isinstance(entry, str):
            entry = entry.decode()

        with open(os.path.join(self.cwd, "search_processed.log"), "a") as writer:
            writer.write(entry)

    def __call__(self, name, param_list, check_for_file=None, **kwargv):
        print("Start:", name)
        self._write_log("------------ " + name + "----------\n")
        self._write_log("System call: " + " ".join(param_list) + "\n")

        start_time = time.time()
        try:
            task = subprocess.run(param_list, cwd=self.cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargv)
        except subprocess.CalledProcessError as e:
            if e.stdout is not None:
                self._write_log(e.stdout)
            if e.stdout is not None:
                self._write_log(e.stderr)
            print(e.stdout.decode())
            raise e

        # write the complete task output to the log file
        if task.stderr is not None:
            self._write_log(task.stderr)
        if task.stdout is not None:
            self._write_log(task.stdout)

        if task.returncode != 0:
            print(task.stdout.decode())
            raise Exception("Error: %s failed" % name)

        if check_for_file:
            if not os.path.isfile(check_for_file):
                print(task.stdout.decode())
                raise Exception("Error: Result file '%s' not found." % check_for_file)

        run_time = time.time() - start_time
        # TODO explore better formatting
        run_time_str = "%d sec" % run_time if run_time < 60 else "%d:%d min" % (run_time // 60, run_time % 60)
        print("    Completed in %s" % run_time_str)


def run(work_dir: str,
        fasta_db_path: str,
        generate_decoy: bool,
        spectra_dir: str,
        precursor_tolerance: float,
        fragment_tolerance: float,
        labelling: str,
        labelling_method: str,
        missed_cleavages: int,
        var_ptms: typing.Tuple[str, ...],
        fixed_ptms: str
        ):
    """
    Generate the decoy database, run the search, and convert the search engine results to TSV files.

    :return: tsv_result_file_path
    """

    # function to callculate paths local to work_dir
    def get_path(*args):
        return os.path.join(work_dir, *args)

    # initialise work_dir for subprocess.run
    task_run = TaskRunner(work_dir)

    # get the free memory in MB
    free_mem = round(psutil.virtual_memory().available / 1024 / 1024)
    # use a minimum of 1G
    if free_mem < 1000:
        free_mem = 1000

    # create the directory paths to work in
    peaklist_abspath = os.path.abspath(spectra_dir)
    if not os.path.isdir(peaklist_abspath):
        raise Exception("Invalid peak list directory selected: " + peaklist_abspath + " does not exist.")

    peptide_shaker_jar_path = "/home/biodocker/bin/PeptideShaker-1.16.27/PeptideShaker-1.16.27.jar"
    searchgui_jar_path = "/home/biodocker/bin/SearchGUI-3.3.3/SearchGUI-3.3.3.jar"

    # the searches should be performed in the "OUT" directory
    if not os.path.isdir(work_dir):
        os.mkdir(work_dir)
    else:
        print("Deleting existing files in " + str(work_dir))
        tmp_files = [f for f in os.listdir(work_dir) if os.path.isfile(os.path.join(work_dir, f))]
        for tmp_file in tmp_files:
            # don't delete the experimental design
            if "exp_design.tsv" in tmp_file or "protocol_parameters.json" in tmp_file:
                continue

            complete_name = os.path.join(work_dir, tmp_file)
            if os.path.isfile(complete_name):
                os.remove(complete_name)

    # -------------------------------------
    # Fix all MGF titles
    print("Adapting MGF titles...")
    mgf_filenames = [os.path.join(peaklist_abspath, f) for f in os.listdir(peaklist_abspath) if
                     f[-4:].lower() == ".mgf"]
    adapt_mgf_titles(mgf_filenames)
    print(mgf_filenames)

    print("Extracting reporter peaks...")
    filter_mgf_peaks(mgf_filenames)

    # -------------------------------------
    # Generate the decoy database

    if generate_decoy:
        print("Creating decoy database...")

        # create the decoy database
        decoy_args = ["java", "-Xmx" + str(free_mem) + "M",
                      "-cp", searchgui_jar_path, "eu.isas.searchgui.cmd.FastaCLI",
                      "-in", fasta_db_path,
                      "-decoy"]

        # get the filename of the decoy database
        database_file = os.path.abspath(fasta_db_path)[:-6] + "_concatenated_target_decoy.fasta"

        task_run("Creating decoy database...", decoy_args, check_for_file=database_file, check=True)
    else:
        # simply use the selected database file
        database_file = os.path.abspath(fasta_db_path)

    if not os.path.isfile(database_file):
        raise Exception("Failed to find generated decoy database")

    # ---------------------------------------------
    # Create the search parameter file

    # path to resulting parameter file
    param_file = get_path("search.par")

    # remove any old parameters
    if os.path.isfile(param_file):
        os.remove(param_file)

    search_args = [
        "java", "-Xmx" + str(free_mem) + "M",
        "-cp", searchgui_jar_path, "eu.isas.searchgui.cmd.IdentificationParametersCLI",
        "-out", param_file,
        "-prec_tol", str(precursor_tolerance),
        "-frag_tol", str(fragment_tolerance),
        "-db", database_file,
        # TODO: labelling cannot always be set as fixed mod???
        "-fixed_mods", str(labelling) + "," + str(fixed_ptms),
        "-mc", str(missed_cleavages),
    ]

    # add variable modifications to search_args
    if len(var_ptms) > 0 or ("Y variable" in labelling_method):
        var_mod_list = []

        for var_mod in var_ptms:
            if var_mod == "Phosphorylation of STY":
                var_mod_list += ["Phosphorylation of S", "Phosphorylation of T", "Phosphorylation of Y"]
            else:
                var_mod_list.append(var_mod)

        if labelling_method == "iTRAQ4 (Y variable)":
            var_mod_list.append("iTRAQ 4-plex of Y")
        if labelling_method == "iTRAQ8 (Y variable)":
            var_mod_list.append("iTRAQ 8-plex of Y")

        search_args.append("-variable_mods")
        search_args.append(",".join(var_mod_list))

    task_run("Creating search parameter file",
             search_args,
             check_for_file=param_file,
             check=True
             )

    # ------------------------------------------------
    # Run the search

    # TODO: create list of spectrum files - or the folder
    spectrum_files = peaklist_abspath
    task_run("SearchCLI",
             ["java", "-Xmx" + str(free_mem) + "M",
              "-cp", searchgui_jar_path, "eu.isas.searchgui.cmd.SearchCLI",
              "-spectrum_files", spectrum_files,
              "-output_folder", work_dir,
              "-id_params", param_file,
              "-xtandem", "0",
              "-msgf", "1",
              "-comet", "0",
              "-ms_amanda", "0",
              "-myrimatch", "0",
              "-andromeda", "0",
              "-omssa", "0",
              "-tide", "0"],
             universal_newlines=True
             )

    # -------------------------------------------------
    # Run PeptideShaker

    peptide_shaker_result_file_path = get_path("experiment.cpsx")
    task_run("PeptideShakerCLI processing",
             ["java", "-Xmx" + str(free_mem) + "M",
              "-cp", peptide_shaker_jar_path, "eu.isas.peptideshaker.cmd.PeptideShakerCLI",
              "-useGeneMapping", "0",
              "-experiment", "experiment1",
              "-sample", "test",
              "-replicate", "1",
              "-identification_files", work_dir,
              "-out", peptide_shaker_result_file_path,
              "-id_params", param_file,
              "-spectrum_files", spectrum_files],
             check_for_file=peptide_shaker_result_file_path,
             universal_newlines=True
             )

    # ---------------------------------------------------
    # create TSV output files

    tsv_result_file_path = get_path("experiment1_test_1_Extended_PSM_Report.txt")
    task_run("ReportCLI (conversion to .tsv)",
             ["java", "-Xmx" + str(free_mem) + "M",
              "-cp", peptide_shaker_jar_path, "eu.isas.peptideshaker.cmd.ReportCLI",
              "-in", peptide_shaker_result_file_path,
              "-out_reports", work_dir,
              "-reports", "8"],
             check_for_file=tsv_result_file_path,
             universal_newlines=True
             )

    print("Search Done.")

    return tsv_result_file_path


if __name__ == "__main__":
    main()
